# üß† Sistema de Delibera√ß√£o Assistida por m√∫ltiplos LLMs

Bem-vindo ao **LLMs Debate**, uma plataforma interativa de *Racioc√≠nio Multi-Agente* (Multi-Agent Reasoning) constru√≠da com Next.js 14, React e Tailwind CSS. 

Este projeto permite que voc√™ envie um _prompt_ e observe dezenas de diferentes IAs (Large Language Models) debatendo e refletindo sobre o seu problema em paralelo. O sistema permite rodadas de delibera√ß√£o consecutivas (onde os modelos leem o que os outros especialistas disseram nas rodadas anteriores e melhoram suas pr√≥prias respostas) at√© convergir para uma s√≠ntese estruturada perfeita.

---

## ‚ú® Principais Funcionalidades

- **M√∫ltiplos Provedores de API Integrados:** Suporte ativo nativo para **OpenRouter**, **OpenAI**, **Perplexity** e **Google Gemini** em uma √∫nica interface.
- **26 Modelos Especialistas:** Escolha a dedo qual IA vai fazer parte do seu grupo de delibera√ß√£o. O sistema suporta os melhores raciocinadores do mundo:
   - *Fam√≠lia Claude 4.5 e 4.6 (Opus, Sonnet, Haiku)* (Removidos da OpenRouter; requer chave API direta no futuro)
   - *Fam√≠lia GPT-5 (High, Codex, Pro, Mini)* (Via OpenAI API)
   - *Fam√≠lia Gemini 3 (Pro e Flash com 1M de Tokens)* (Via Google Gemini API)
   - *S√©rie Perplexity Sonar (Pesquisa web ao vivo)* (Via Perplexity API)
   - *5 Gigantes 100% Gratuitos via OpenRouter:* Qwen Next 80B Instruct, Llama 3.3 70B Instruct, Qwen3 VL Thinking (30B), Upstage Solar Pro 3 e Liquid LFM-2.5 1.2b Thinking.
- **Reflex√£o por Rodadas (Aprofundamento):** Os modelos n√£o respondem apenas uma vez. Voc√™ pode iniciar a "Rodada 2", onde o sistema injeta as respostas de todos os especialistas da rodada passada no contexto, for√ßando-os a repensar suas ideias com base nas cr√≠ticas uns dos outros.
- **Transcri√ß√£o e S√≠ntese Final:** Exporte toda a cadeia de racioc√≠nio da delibera√ß√£o em formato `.MD` com o clique de um bot√£o.
- **Modelos Locais Offline:** O sistema consegue varrer automaticamente o seu **LM Studio** na porta `1234` e adicionar modelos locais executando diretos do seu equipamento (ex: Llama 3 70B, Qwen, etc) para participarem das rodadas sem custo de nuvem.

---

## üöÄ Como instalar e rodar

1. Clone o reposit√≥rio em sua m√°quina:
   ```bash
   git clone https://github.com/darlanvsvs/llmsdebate.git
   cd llmsdebate
   ```

2. Instale as depend√™ncias atrav√©s do NPM ou do gerenciador de sua prefer√™ncia:
   ```bash
   npm install
   ```

3. Modifique o nome do arquivo `.env.example` (se houver) para `.env.local` e preencha as suas chaves de API:
   ```env
   OPENAI_API_KEY="sua_chave_aqui"
   OPENROUTER_API_KEY="sua_chave_aqui"
   GEMINI_API_KEY="sua_chave_aqui"
   PERPLEXITY_API_KEY="sua_chave_aqui"
   ```

4. Inicialize o servidor de desenvolvimento:
   ```bash
   npm run dev
   ```

5. O aplicativo estar√° rodando em `http://localhost:3000`.

---

## üõ†Ô∏è Stack Tecnol√≥gica

- **Frontend:** Next.js 14 (App Router), React 18, Zustand (State Management LocalStorage), Tailwind CSS (Glassmorphism UI), Lucide Icons.
- **Backend:** Rotas de API Edge via Next.js lidando com parse, stream e mapeamento estrito para os 4 provedores diferentes.

---

## üé® Sobre a Interface

O design foca em imers√£o com tons escuros profundos inspirados no universo espacial (efeitos glassmorphism, translucidez com desfoque de cen√°rio `backdrop-blur`). As respostas dos especialistas s√£o fragmentadas automaticamente pelo sistema em duas fases visuais obrigat√≥rias: **An√°lise** e **Conclus√£o Final**, facilitando a leitura e compara√ß√£o instant√¢nea do racioc√≠nio anal√≠tico com o veredicto daquele modelo para o usu√°rio final.

You can check out [the Next.js GitHub repository](https://github.com/vercel/next.js) - your feedback and contributions are welcome!

## Deploy on Vercel


Check out our [Next.js deployment documentation](https://nextjs.org/docs/app/building-your-application/deploying) for more details.
