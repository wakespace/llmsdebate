# üß† Sistema de Delibera√ß√£o Assistida por m√∫ltiplos LLMs

Bem-vindo ao **LLMs Debate**, uma plataforma interativa de *Racioc√≠nio Multi-Agente* (Multi-Agent Reasoning) constru√≠da com Next.js 14, React e Tailwind CSS. 

Este projeto permite que voc√™ envie um _prompt_ e observe dezenas de diferentes IAs (Large Language Models) debatendo e refletindo sobre o seu problema em paralelo. O sistema permite rodadas de delibera√ß√£o consecutivas (onde os modelos leem o que os outros especialistas disseram nas rodadas anteriores e melhoram suas pr√≥prias respostas) at√© convergir para uma s√≠ntese estruturada perfeita.

---

## ‚ú® Principais Funcionalidades

- **M√∫ltiplos Provedores de API Integrados:** Suporte ativo nativo para **OpenRouter**, **OpenAI**, **Perplexity** e **Google Gemini** em uma √∫nica interface.
- **Atualiza√ß√£o Din√¢mica de Modelos:** Uma pipeline automatizada via GitHub Actions e Node.js atualiza regularmente os modelos suportados puxando do registro da OpenRouter, com suporte simult√¢neo a chaves da OpenAI, Gemini, Perplexity e Anthropic.
- **M√∫ltiplos Modelos Especialistas:** Escolha a dedo qual IA far√° parte do seu grupo de delibera√ß√£o a partir de uma lista expansiva e atualizada.
- **Reflex√£o por Rodadas (Aprofundamento):** Os modelos n√£o respondem apenas uma vez. Voc√™ pode iniciar novas rodadas onde o sistema injeta as respostas de todos os especialistas da rodada passada no contexto, for√ßando-os a repensar suas vis√µes.
- **Juiz Integrado (Judge LLM):** Utiliza um LLM superior como "Juiz" para ler o hist√≥rico da delibera√ß√£o, comparar argumentos, resolver diverg√™ncias e construir um consenso bem elaborado.
- **Interface e Configura√ß√µes Refinadas:** Nova barra lateral que agrupa par√¢metros de controle, configura√ß√µes das APIs para provedores variados e personaliza√ß√£o do layout da delibera√ß√£o de maneira intuitiva.
- **Transcri√ß√£o e S√≠ntese Final:** Exporte toda a cadeia de racioc√≠nio da delibera√ß√£o em formato `.MD` com o clique de um bot√£o.
- **Modelos Locais Offline:** O sistema consegue varrer a porta local para identificar inst√¢ncias do seu **LM Studio** e adicionar IAs diretos do seu equipamento (ex: Llama 3, Qwen) como participantes das rodadas de debate gr√°tis e sem lat√™ncia.

---

## üöÄ Como instalar e rodar

1. Clone o reposit√≥rio em sua m√°quina:
   ```bash
   git clone https://github.com/darlanvsvs/llmsdebate.git
   cd llmsdebate
   ```

2. Instale as depend√™ncias atrav√©s do NPM ou do gerenciador de sua prefer√™ncia:
   ```bash
   npm install
   ```

3. Modifique o nome do arquivo `.env.example` (se houver) para `.env.local` e preencha as suas chaves de API:
   ```env
   OPENAI_API_KEY="sua_chave_aqui"
   OPENROUTER_API_KEY="sua_chave_aqui"
   GEMINI_API_KEY="sua_chave_aqui"
   PERPLEXITY_API_KEY="sua_chave_aqui"
   ```

4. Inicialize o servidor de desenvolvimento:
   ```bash
   npm run dev
   ```

5. O aplicativo estar√° rodando em `http://localhost:3000`.

---

## üõ†Ô∏è Stack Tecnol√≥gica

- **Frontend:** Next.js 14 (App Router), React 18, Zustand (State Management LocalStorage), Tailwind CSS (Glassmorphism UI), Lucide Icons.
- **Backend:** Rotas de API Edge via Next.js lidando com parse, stream e mapeamento estrito para os 4 provedores diferentes.

---

## üé® Sobre a Interface

O design foca em imers√£o com tons escuros profundos inspirados no universo espacial (efeitos glassmorphism, translucidez com desfoque de cen√°rio `backdrop-blur`). As respostas dos especialistas s√£o fragmentadas automaticamente pelo sistema em duas fases visuais obrigat√≥rias: **An√°lise** e **Conclus√£o Final**, facilitando a leitura e compara√ß√£o instant√¢nea do racioc√≠nio anal√≠tico com o veredicto daquele modelo para o usu√°rio final.

You can check out [the Next.js GitHub repository](https://github.com/vercel/next.js) - your feedback and contributions are welcome!

## Deploy on Vercel


Check out our [Next.js deployment documentation](https://nextjs.org/docs/app/building-your-application/deploying) for more details.
